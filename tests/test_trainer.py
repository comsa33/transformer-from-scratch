"""
Trainer í…ŒìŠ¤íŠ¸ ë° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜
"""

import sys

sys.path.append(".")

import os
import shutil

import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.utils.data import Dataset

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams["font.family"] = "NanumGothic"
plt.rcParams["axes.unicode_minus"] = False

from training.trainer import Trainer, TrainingConfig
from transformer.models.transformer import create_transformer_small


class DummyTranslationDataset(Dataset):
    """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë²ˆì—­ ë°ì´í„°ì…‹"""

    def __init__(
        self, size: int = 1000, src_len: int = 20, tgt_len: int = 25, vocab_size: int = 1000
    ):
        self.size = size
        self.src_len = src_len
        self.tgt_len = tgt_len
        self.vocab_size = vocab_size

        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        torch.manual_seed(42)
        self.src_ids = torch.randint(1, vocab_size, (size, src_len))
        self.tgt_ids = torch.randint(1, vocab_size, (size, tgt_len))

        # ì¼ë¶€ íŒ¨ë”© ì¶”ê°€
        for i in range(size):
            pad_len = np.random.randint(0, 5)
            if pad_len > 0:
                self.src_ids[i, -pad_len:] = 0
                self.tgt_ids[i, -pad_len:] = 0

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return {
            "src_ids": self.src_ids[idx],
            "tgt_ids": self.tgt_ids[idx],
            "labels": self.tgt_ids[idx],
        }


def custom_data_collator(features: list[dict]) -> dict[str, torch.Tensor]:
    """ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„"""
    batch = {}

    # ê° í‚¤ë³„ë¡œ í…ì„œ ìŠ¤íƒ
    for key in features[0]:
        batch[key] = torch.stack([f[key] for f in features])

    return batch


def compute_metrics(eval_pred: tuple[torch.Tensor, torch.Tensor]) -> dict[str, float]:
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    predictions, labels = eval_pred

    # ì •í™•ë„ ê³„ì‚°
    mask = labels != -100  # padding ì œì™¸
    correct = (predictions == labels) & mask
    accuracy = correct.sum().float() / mask.sum().float()

    return {"accuracy": accuracy.item()}


class LoggingCallback:
    """ë¡œê¹… ì½œë°± ì˜ˆì œ"""

    def on_train_begin(self, trainer, state, model):
        print("ğŸš€ Training started!")

    def on_epoch_begin(self, trainer, state, model):
        print(f"\nğŸ“… Epoch {state.epoch + 1} started")

    def on_epoch_end(self, trainer, state, model):
        print(f"âœ… Epoch {state.epoch + 1} completed")

    def on_train_end(self, trainer, state, model):
        print("ğŸ‰ Training completed!")

    def on_evaluate(self, trainer, state, model):
        print("ğŸ“Š Running evaluation...")


def test_basic_training():
    """ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    print("=== ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ===\n")

    # ì‘ì€ ëª¨ë¸ê³¼ ë°ì´í„°ì…‹
    model = create_transformer_small(
        src_vocab_size=1000,
        tgt_vocab_size=1000,
        max_length=100,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_model=128,
        num_heads=4,
    )

    # ë°ì´í„°ì…‹
    train_dataset = DummyTranslationDataset(size=200)
    eval_dataset = DummyTranslationDataset(size=50)

    # í•™ìŠµ ì„¤ì •
    training_args = TrainingConfig(
        output_dir="./test_checkpoints",
        num_train_epochs=2,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=32,
        learning_rate=1e-3,
        warmup_steps=10,
        logging_steps=5,
        eval_steps=15,
        save_steps=20,
        fp16=False,  # CPUì—ì„œ í…ŒìŠ¤íŠ¸
    )

    # Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=custom_data_collator,
        compute_metrics=None,  # ì¼ë‹¨ ë©”íŠ¸ë¦­ ê³„ì‚° ì œì™¸
        callbacks=[LoggingCallback()],
    )

    # í•™ìŠµ
    print("Starting training...")
    history = trainer.train()

    # ê²°ê³¼ ì¶œë ¥
    print(f"\ní•™ìŠµ ì™„ë£Œ! ì´ ìŠ¤í…: {trainer.state.global_step}")

    # ì •ë¦¬
    if os.path.exists("./test_checkpoints"):
        shutil.rmtree("./test_checkpoints")

    return history


def test_gradient_accumulation():
    """Gradient Accumulation í…ŒìŠ¤íŠ¸"""
    print("\n=== Gradient Accumulation í…ŒìŠ¤íŠ¸ ===\n")

    model = create_transformer_small(
        src_vocab_size=100,
        tgt_vocab_size=100,
        num_encoder_layers=1,
        num_decoder_layers=1,
        d_model=64,
    )

    train_dataset = DummyTranslationDataset(size=64, vocab_size=100)

    # ë‹¤ì–‘í•œ accumulation steps í…ŒìŠ¤íŠ¸
    accumulation_steps = [1, 2, 4, 8]
    results = {}

    for acc_steps in accumulation_steps:
        training_args = TrainingConfig(
            output_dir=f"./test_acc_{acc_steps}",
            num_train_epochs=1,
            per_device_train_batch_size=8,
            gradient_accumulation_steps=acc_steps,
            learning_rate=1e-3,
            logging_steps=5,
            eval_steps=-1,  # í‰ê°€ ì•ˆí•¨
            save_steps=-1,  # ì €ì¥ ì•ˆí•¨
        )

        trainer = Trainer(
            model=model.to("cpu"),  # ëª¨ë¸ ì´ˆê¸°í™”
            args=training_args,
            train_dataset=train_dataset,
            data_collator=custom_data_collator,
        )

        history = trainer.train()

        # ì‹¤ì œ ë°°ì¹˜ í¬ê¸°
        effective_batch_size = 8 * acc_steps
        results[acc_steps] = {
            "effective_batch_size": effective_batch_size,
            "final_loss": history[-1]["loss"] if history else 0,
        }

        # ì •ë¦¬
        if os.path.exists(f"./test_acc_{acc_steps}"):
            shutil.rmtree(f"./test_acc_{acc_steps}")

    print("Gradient Accumulation ê²°ê³¼:")
    print("-" * 50)
    print("Acc Steps | Effective Batch | Final Loss")
    print("-" * 50)
    for acc_steps, res in results.items():
        print(f"{acc_steps:9d} | {res['effective_batch_size']:15d} | {res['final_loss']:.4f}")

    return results


def test_scheduler_integration():
    """Scheduler í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n=== Scheduler í†µí•© í…ŒìŠ¤íŠ¸ ===\n")

    model = create_transformer_small(src_vocab_size=100, tgt_vocab_size=100, d_model=128)

    train_dataset = DummyTranslationDataset(size=100, vocab_size=100)

    # ë‹¤ì–‘í•œ scheduler í…ŒìŠ¤íŠ¸
    schedulers = ["linear", "cosine", "transformer"]
    lr_histories = {}

    for scheduler_type in schedulers:
        training_args = TrainingConfig(
            output_dir=f"./test_scheduler_{scheduler_type}",
            num_train_epochs=2,
            per_device_train_batch_size=10,
            learning_rate=1e-3,
            lr_scheduler_type=scheduler_type,
            warmup_steps=20,
            logging_steps=5,
        )

        trainer = Trainer(
            model=model.to("cpu"),
            args=training_args,
            train_dataset=train_dataset,
            data_collator=custom_data_collator,
        )

        # LR ì¶”ì ì„ ìœ„í•œ ìˆ˜ì •
        lrs = []
        original_train = trainer.train

        def track_lr():
            history = original_train()
            # ë¡œê·¸ì—ì„œ LR ì¶”ì¶œ
            for log in history:
                if "learning_rate" in log:
                    lrs.append(log["learning_rate"])
            return history

        trainer.train = track_lr
        trainer.train()

        lr_histories[scheduler_type] = lrs

        # ì •ë¦¬
        if os.path.exists(f"./test_scheduler_{scheduler_type}"):
            shutil.rmtree(f"./test_scheduler_{scheduler_type}")

    # LR ê³¡ì„  ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    for scheduler_type, lrs in lr_histories.items():
        plt.plot(lrs, label=scheduler_type, linewidth=2)

    plt.xlabel("Step")
    plt.ylabel("Learning Rate")
    plt.title("Learning Rate Schedules in Training")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig("outputs/trainer_lr_schedules.png", dpi=150)
    print("LR ìŠ¤ì¼€ì¤„ì´ 'outputs/trainer_lr_schedules.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return lr_histories


def test_checkpoint_save_load():
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("\n=== ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸ ===\n")

    # ëª¨ë¸ê³¼ ë°ì´í„°
    model = create_transformer_small(src_vocab_size=100, tgt_vocab_size=100, d_model=64)
    train_dataset = DummyTranslationDataset(size=100, vocab_size=100)

    # ì²« ë²ˆì§¸ í•™ìŠµ (ì²´í¬í¬ì¸íŠ¸ ì €ì¥)
    training_args = TrainingConfig(
        output_dir="./checkpoint_test",
        num_train_epochs=2,
        per_device_train_batch_size=10,
        save_steps=5,
        logging_steps=5,
        save_total_limit=2,
    )

    trainer1 = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=custom_data_collator,
    )

    trainer1.train()
    final_step1 = trainer1.state.global_step

    print(f"ì²« ë²ˆì§¸ í•™ìŠµ ì™„ë£Œ. ìµœì¢… ìŠ¤í…: {final_step1}")

    # ì²´í¬í¬ì¸íŠ¸ í™•ì¸
    checkpoints = [d for d in os.listdir("./checkpoint_test") if d.startswith("checkpoint-")]
    print(f"ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸: {checkpoints}")

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    if checkpoints:
        latest_checkpoint = sorted(checkpoints, key=lambda x: int(x.split("-")[1]))[-1]
        checkpoint_path = os.path.join("./checkpoint_test", latest_checkpoint)

        # ìƒˆ ëª¨ë¸ê³¼ trainer
        model2 = create_transformer_small(src_vocab_size=100, tgt_vocab_size=100, d_model=64)

        training_args2 = TrainingConfig(
            output_dir="./checkpoint_test",
            num_train_epochs=3,  # ì¶”ê°€ epoch
            per_device_train_batch_size=10,
            resume_from_checkpoint=checkpoint_path,
            logging_steps=5,
        )

        trainer2 = Trainer(
            model=model2,
            args=training_args2,
            train_dataset=train_dataset,
            data_collator=custom_data_collator,
        )

        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë° í•™ìŠµ ì¬ê°œ
        trainer2._load_checkpoint(checkpoint_path)
        print(f"\nì²´í¬í¬ì¸íŠ¸ '{latest_checkpoint}'ì—ì„œ ì¬ê°œ")
        print(f"ì¬ê°œ ì‹œì  ìŠ¤í…: {trainer2.state.global_step}")

        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¹„êµ
        state1 = model.state_dict()
        state2 = model2.state_dict()

        weight_diff = 0
        for key in state1:
            if key in state2:
                diff = (state1[key] - state2[key]).abs().mean().item()
                weight_diff += diff

        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í›„ ê°€ì¤‘ì¹˜ ì°¨ì´: {weight_diff:.6f}")

    # ì •ë¦¬
    if os.path.exists("./checkpoint_test"):
        shutil.rmtree("./checkpoint_test")

    return checkpoints


def test_mixed_precision():
    """Mixed Precision í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    print("\n=== Mixed Precision í…ŒìŠ¤íŠ¸ ===\n")

    if not torch.cuda.is_available():
        print("CUDAê°€ ì—†ì–´ì„œ Mixed Precision í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    model = create_transformer_small(src_vocab_size=100, tgt_vocab_size=100, d_model=128)

    train_dataset = DummyTranslationDataset(size=100, vocab_size=100)

    # FP16 vs FP32 ë¹„êµ
    results = {}

    for use_fp16 in [False, True]:
        training_args = TrainingConfig(
            output_dir=f"./fp16_test_{use_fp16}",
            num_train_epochs=1,
            per_device_train_batch_size=16,
            fp16=use_fp16,
            logging_steps=10,
        )

        trainer = Trainer(
            model=model.cuda(),
            args=training_args,
            train_dataset=train_dataset,
            data_collator=custom_data_collator,
        )

        import time

        start_time = time.time()
        history = trainer.train()
        train_time = time.time() - start_time

        results[f"FP{16 if use_fp16 else 32}"] = {
            "time": train_time,
            "final_loss": history[-1]["loss"] if history else 0,
        }

        # ì •ë¦¬
        if os.path.exists(f"./fp16_test_{use_fp16}"):
            shutil.rmtree(f"./fp16_test_{use_fp16}")

    print("Mixed Precision ê²°ê³¼:")
    for precision, res in results.items():
        print(f"{precision}: ì‹œê°„={res['time']:.2f}s, Loss={res['final_loss']:.4f}")


def test_training_config():
    """TrainingConfig ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    print("\n=== TrainingConfig í…ŒìŠ¤íŠ¸ ===\n")

    # ì„¤ì • ìƒì„±
    config = TrainingConfig(
        output_dir="./config_test",
        num_train_epochs=10,
        per_device_train_batch_size=32,
        learning_rate=5e-5,
        warmup_ratio=0.1,
        fp16=True,
        gradient_checkpointing=True,
        custom_param="test_value",  # ì¶”ê°€ íŒŒë¼ë¯¸í„°
    )

    # JSONìœ¼ë¡œ ì €ì¥
    os.makedirs("./config_test", exist_ok=True)
    config.save_to_json("./config_test/config.json")

    # ë¡œë“œ
    loaded_config = TrainingConfig.from_json("./config_test/config.json")

    # ë¹„êµ
    print("ì›ë³¸ ì„¤ì •:")
    print(f"  learning_rate: {config.learning_rate}")
    print(f"  warmup_ratio: {config.warmup_ratio}")
    print(f"  custom_param: {config.custom_param}")

    print("\në¡œë“œëœ ì„¤ì •:")
    print(f"  learning_rate: {loaded_config.learning_rate}")
    print(f"  warmup_ratio: {loaded_config.warmup_ratio}")
    print(f"  custom_param: {loaded_config.custom_param}")

    # ì •ë¦¬
    if os.path.exists("./config_test"):
        shutil.rmtree("./config_test")


def visualize_training_history(history: list[dict]):
    """í•™ìŠµ ì´ë ¥ ì‹œê°í™”"""
    if not history:
        return

    # ë°ì´í„° ì¶”ì¶œ
    steps = [log["step"] for log in history if "loss" in log]
    losses = [log["loss"] for log in history if "loss" in log]
    lrs = [log["learning_rate"] for log in history if "learning_rate" in log]

    eval_steps = [log["step"] for log in history if "eval_loss" in log]
    eval_losses = [log["eval_loss"] for log in history if "eval_loss" in log]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

    # Loss ê·¸ë˜í”„
    ax1.plot(steps, losses, "b-", label="Train Loss", linewidth=2)
    if eval_steps:
        ax1.plot(eval_steps, eval_losses, "r--", label="Eval Loss", linewidth=2, marker="o")
    ax1.set_xlabel("Step")
    ax1.set_ylabel("Loss")
    ax1.set_title("Training and Evaluation Loss")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Learning Rate ê·¸ë˜í”„
    ax2.plot(steps[: len(lrs)], lrs, "g-", linewidth=2)
    ax2.set_xlabel("Step")
    ax2.set_ylabel("Learning Rate")
    ax2.set_title("Learning Rate Schedule")
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("outputs/training_history.png", dpi=150)
    print("\ní•™ìŠµ ì´ë ¥ì´ 'outputs/training_history.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # 1. ê¸°ë³¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
    history = test_basic_training()

    # 2. Gradient Accumulation í…ŒìŠ¤íŠ¸
    grad_acc_results = test_gradient_accumulation()

    # 3. Scheduler í†µí•© í…ŒìŠ¤íŠ¸
    lr_histories = test_scheduler_integration()

    # 4. ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
    checkpoints = test_checkpoint_save_load()

    # 5. Mixed Precision í…ŒìŠ¤íŠ¸
    test_mixed_precision()

    # 6. TrainingConfig í…ŒìŠ¤íŠ¸
    test_training_config()

    # 7. í•™ìŠµ ì´ë ¥ ì‹œê°í™”
    if history:
        visualize_training_history(history)

    print("\nëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
