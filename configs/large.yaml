# Large Transformer 설정
# 더 큰 성능을 위한 대규모 모델

model:
  d_model: 1024
  num_heads: 16
  num_encoder_layers: 12
  num_decoder_layers: 12
  d_ff: 4096
  max_seq_length: 1024
  dropout_rate: 0.1
  attention_dropout_rate: 0.1
  activation: "gelu"  # 더 나은 성능을 위해 GELU 사용
  layer_norm_eps: 1e-6
  initializer_range: 0.02
  use_bias: true
  share_embeddings: true  # 파라미터 효율성을 위해 임베딩 공유

training:
  batch_size: 16
  learning_rate: 0.00005
  warmup_steps: 8000
  max_steps: 500000
  gradient_clip_val: 1.0
  label_smoothing: 0.1
  gradient_accumulation_steps: 4  # 효과적인 배치 크기 = 16 * 4 = 64

optimizer:
  type: "adamw"
  betas: [0.9, 0.98]
  eps: 1e-9
  weight_decay: 0.01

scheduler:
  type: "transformer"
  warmup_steps: 8000
  d_model: 1024

data:
  max_length: 1024
  num_workers: 8
  pin_memory: true
  prefetch_factor: 2

logging:
  log_interval: 100
  eval_interval: 2000
  save_interval: 10000

checkpoint:
  save_dir: "checkpoints_large"
  save_total_limit: 10
  save_best: true

# 추가 학습 기법
training_techniques:
  mixed_precision: true  # FP16 학습
  gradient_checkpointing: true  # 메모리 절약
  stochastic_depth_rate: 0.1  # Stochastic depth for regularization
