# Transformer ëª¨ë¸ êµ¬í˜„ í”„ë¡œì íŠ¸

## ğŸ“Š í”„ë¡œì íŠ¸ ë¶„ì„ ë¦¬í¬íŠ¸
ë³¸ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  êµ¬í˜„ ê²°ê³¼ì™€ ë…¼ë¬¸ ì¬í˜„ ê²€ì¦ ë‚´ìš©ì„ ë‹´ì€ **[ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸](FINAL_REPORT.md)** ë¥¼ í™•ì¸í•˜ì„¸ìš”. ë¦¬í¬íŠ¸ì—ëŠ” 30ê°œ ì´ìƒì˜ ì‹œê°í™” ê²°ê³¼ì™€ í•¨ê»˜ ê° êµ¬ì„± ìš”ì†Œì˜ ìƒì„¸í•œ ë¶„ì„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ê°œìš”
ì´ í”„ë¡œì íŠ¸ëŠ” Transformer ì•„í‚¤í…ì²˜ë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•˜ë©° ê° êµ¬ì„± ìš”ì†Œë¥¼ ê¹Šì´ ì´í•´í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. Python 3.11ê³¼ uv íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

## Transformer ì•„í‚¤í…ì²˜ ìƒì„¸

### 1. ì „ì²´ êµ¬ì¡° ê°œìš”

TransformerëŠ” 2017ë…„ ["Attention is All You Need"](https://arxiv.org/abs/1706.03762) ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œëœ ëª¨ë¸ë¡œ, RNNì´ë‚˜ CNN ì—†ì´ ì˜¤ì§ Attention ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ êµ¬ì„±ëœ í˜ì‹ ì ì¸ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

```
ì…ë ¥ ì‹œí€€ìŠ¤ â†’ Encoder â†’ Context Vectors â†’ Decoder â†’ ì¶œë ¥ ì‹œí€€ìŠ¤
```

### 2. í•µì‹¬ êµ¬ì„± ìš”ì†Œ

#### 2.1 Multi-Head Attention

Multi-Head Attentionì€ Transformerì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ê°œì˜ Attention Headë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ representation subspaceì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

**êµ¬ì„± ìš”ì†Œ:**
- **Query (Q)**: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ì°¾ê³ ì í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°
- **Key (K)**: ê° ìœ„ì¹˜ì˜ ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°
- **Value (V)**: ì‹¤ì œë¡œ ì „ë‹¬ë  ì •ë³´ë¥¼ ë‹´ì€ ë²¡í„°

**ìˆ˜ì‹:**
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V

MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**íŒŒë¼ë¯¸í„°:**
- `d_model`: ëª¨ë¸ì˜ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 512)
- `num_heads`: Attention Headì˜ ê°œìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 8)
- `d_k = d_v = d_model / num_heads`: ê° Headì˜ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 64)

#### 2.2 Position-wise Feed-Forward Networks (FFN)

ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì™„ì „ ì—°ê²° ì‹ ê²½ë§ì…ë‹ˆë‹¤.

**êµ¬ì¡°:**
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```

**íŠ¹ì§•:**
- ë‘ ê°œì˜ Linear Transformationìœ¼ë¡œ êµ¬ì„±
- ì¤‘ê°„ì— ReLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
- Hidden Layer ì°¨ì›ì€ ì¼ë°˜ì ìœ¼ë¡œ `d_model`ì˜ 4ë°° (2048)

#### 2.3 Positional Encoding

TransformerëŠ” ìˆœì„œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ìœ„ì¹˜ ì •ë³´ë¥¼ ì…ë ¥ì— ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

**ìˆ˜ì‹:**
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

ì—¬ê¸°ì„œ:
- `pos`: ì‹œí€€ìŠ¤ ë‚´ ìœ„ì¹˜
- `i`: ì°¨ì› ì¸ë±ìŠ¤

#### 2.4 Layer Normalization

ê° sub-layer ì¶œë ¥ì— ì ìš©ë˜ì–´ í•™ìŠµì„ ì•ˆì •í™”í•©ë‹ˆë‹¤.

**ìˆ˜ì‹:**
```
LayerNorm(x) = Î³ * (x - Î¼) / Ïƒ + Î²
```

ì—¬ê¸°ì„œ:
- `Î¼`: í‰ê· 
- `Ïƒ`: í‘œì¤€í¸ì°¨
- `Î³, Î²`: í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°

### 3. Encoder êµ¬ì¡°

EncoderëŠ” Nê°œì˜ ë™ì¼í•œ ì¸µìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤ (ì¼ë°˜ì ìœ¼ë¡œ N=6).

**ê° Encoder Layer êµ¬ì„±:**
1. **Multi-Head Self-Attention**
   - ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ëª¨ë“  ìœ„ì¹˜ë¥¼ ì°¸ì¡°
   - Residual Connection + Layer Normalization

2. **Position-wise Feed-Forward Network**
   - ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš©
   - Residual Connection + Layer Normalization

**Encoder ì „ì²´ êµ¬ì¡°:**
```
Input Embedding â†’ Positional Encoding â†’
[Encoder Layer 1] â†’ [Encoder Layer 2] â†’ ... â†’ [Encoder Layer N] â†’
Encoder Output
```

### 4. Decoder êµ¬ì¡°

Decoderë„ Nê°œì˜ ë™ì¼í•œ ì¸µìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

**ê° Decoder Layer êµ¬ì„±:**
1. **Masked Multi-Head Self-Attention**
   - ë¯¸ë˜ ìœ„ì¹˜ì˜ ì •ë³´ë¥¼ ì°¸ì¡°í•˜ì§€ ëª»í•˜ë„ë¡ ë§ˆìŠ¤í‚¹
   - Residual Connection + Layer Normalization

2. **Multi-Head Cross-Attention**
   - QueryëŠ” Decoderì—ì„œ, Keyì™€ ValueëŠ” Encoder ì¶œë ¥ì—ì„œ ê°€ì ¸ì˜´
   - Residual Connection + Layer Normalization

3. **Position-wise Feed-Forward Network**
   - Encoderì™€ ë™ì¼í•œ êµ¬ì¡°
   - Residual Connection + Layer Normalization

**Decoder ì „ì²´ êµ¬ì¡°:**
```
Output Embedding â†’ Positional Encoding â†’
[Decoder Layer 1] â†’ [Decoder Layer 2] â†’ ... â†’ [Decoder Layer N] â†’
Linear â†’ Softmax â†’ Output Probabilities
```

### 5. í•™ìŠµ ì„¸ë¶€ì‚¬í•­

#### 5.1 ì†ì‹¤ í•¨ìˆ˜
- **Cross-Entropy Loss**: ë‹¤ìŒ í† í° ì˜ˆì¸¡ì„ ìœ„í•œ í‘œì¤€ ì†ì‹¤ í•¨ìˆ˜

#### 5.2 ìµœì í™”
- **Optimizer**: Adam (Î²â‚=0.9, Î²â‚‚=0.98, Îµ=10â»â¹)
- **Learning Rate Schedule**: Warmup ê¸°ë°˜ ìŠ¤ì¼€ì¤„
  ```
  lrate = d_model^(-0.5) * min(step_num^(-0.5), step_num * warmup_steps^(-1.5))
  ```

#### 5.3 ì •ê·œí™” ê¸°ë²•
- **Dropout**: 0.1 (ê° sub-layer ì¶œë ¥ì— ì ìš©)
- **Label Smoothing**: Îµ=0.1

### 6. êµ¬í˜„ ëª¨ë“ˆ êµ¬ì¡°

í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª¨ë“ˆ êµ¬ì¡°:

```
test-modeling/
â”œâ”€â”€ transformer/          # Transformer êµ¬í˜„ ì½”ë“œ
â”‚   â”œâ”€â”€ embeddings/      # ì„ë² ë”© ê´€ë ¨ ëª¨ë“ˆ
â”‚   â”‚   â”œâ”€â”€ positional.py
â”‚   â”‚   â””â”€â”€ token_embedding.py
â”‚   â”œâ”€â”€ layers/          # ë ˆì´ì–´ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”‚   â”œâ”€â”€ normalization.py
â”‚   â”‚   â””â”€â”€ residual.py
â”‚   â”œâ”€â”€ models/          # ì „ì²´ ëª¨ë¸ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ encoder.py
â”‚   â”‚   â”œâ”€â”€ decoder.py
â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â””â”€â”€ utils/           # ìœ í‹¸ë¦¬í‹°
â”‚       â””â”€â”€ masking.py
â”œâ”€â”€ tests/               # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ test_attention.py
â”‚   â”œâ”€â”€ test_decoder.py
â”‚   â”œâ”€â”€ test_encoder.py
â”‚   â”œâ”€â”€ test_feedforward.py
â”‚   â”œâ”€â”€ test_layer_normalization.py
â”‚   â”œâ”€â”€ test_masking.py
â”‚   â”œâ”€â”€ test_positional_encoding.py
â”‚   â”œâ”€â”€ test_residual.py
â”‚   â”œâ”€â”€ test_token_embedding.py
â”‚   â””â”€â”€ test_transformer.py
â””â”€â”€ outputs/             # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ì‹œê°í™”
    â”œâ”€â”€ attention_patterns.png
    â”œâ”€â”€ decoder_attention_patterns.png
    â”œâ”€â”€ encoder_attention_patterns.png
    â””â”€â”€ ... (ê¸°íƒ€ ì‹œê°í™” ê²°ê³¼)
```

### 7. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `d_model` | 512 | ëª¨ë¸ì˜ ì°¨ì› |
| `num_heads` | 8 | Attention Head ê°œìˆ˜ |
| `num_layers` | 6 | Encoder/Decoder ì¸µ ìˆ˜ |
| `d_ff` | 2048 | FFNì˜ Hidden Layer ì°¨ì› |
| `max_seq_length` | 512 | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ |
| `vocab_size` | ê°€ë³€ | ì–´íœ˜ í¬ê¸° |
| `dropout_rate` | 0.1 | Dropout ë¹„ìœ¨ |

## í”„ë¡œì íŠ¸ ì‹¤í–‰ ë°©ë²•

### ì˜ì¡´ì„± ì„¤ì¹˜
```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (uv ì‚¬ìš©)
uv sync

# ê°œë°œ í™˜ê²½ ì„¤ì • (pre-commit ì„¤ì¹˜)
uv run pre-commit install
```

### ì½”ë“œ í’ˆì§ˆ ê´€ë¦¬

ì´ í”„ë¡œì íŠ¸ëŠ” pre-commit hooksë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œ í’ˆì§ˆì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

- **Black**: ì½”ë“œ í¬ë§·íŒ… (line-length: 100)
- **Ruff**: ë¹ ë¥¸ Python linter
- **MyPy**: ì •ì  íƒ€ì… ê²€ì‚¬
- **uv-lock**: ì˜ì¡´ì„± ë™ê¸°í™”

ì»¤ë°‹ ì „ ëª¨ë“  íŒŒì¼ ê²€ì‚¬:
```bash
uv run pre-commit run --all-files
```

### Configuration ì‹œìŠ¤í…œ

í”„ë¡œì íŠ¸ëŠ” YAML ê¸°ë°˜ì˜ ì„¤ì • ì‹œìŠ¤í…œì„ ì‚¬ìš©í•©ë‹ˆë‹¤. `configs/` ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ì „ ì •ì˜ëœ ì„¤ì •ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `base.yaml`: ë…¼ë¬¸ ê¸°ì¤€ í‘œì¤€ ì„¤ì • (d_model=512, 6 layers)
- `small.yaml`: ë¹ ë¥¸ ì‹¤í—˜ìš© ì‘ì€ ëª¨ë¸ (d_model=256, 3 layers)
- `large.yaml`: ê³ ì„±ëŠ¥ ëŒ€ê·œëª¨ ëª¨ë¸ (d_model=1024, 12 layers)
- `debug.yaml`: ë””ë²„ê¹…ìš© ìµœì†Œ ì„¤ì • (d_model=128, 2 layers)

### í•™ìŠµ ì‹¤í–‰
```bash
# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
uv run scripts/train_with_config.py

# íŠ¹ì • ì„¤ì • íŒŒì¼ ì‚¬ìš©
uv run scripts/train_with_config.py --config small

# ì„¤ì • íŒŒì¼ + ëª…ë ¹ì¤„ ì˜µì…˜
uv run scripts/train_with_config.py --config base --batch-size 64 --learning-rate 0.0005
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰
# (test-modeling ë””ë ‰í† ë¦¬ì—ì„œ)

# ê° ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run tests/test_positional_encoding.py
uv run tests/test_token_embedding.py
uv run tests/test_attention.py
uv run tests/test_transformer.py
# ... ê¸°íƒ€ í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤

# ê²°ê³¼ëŠ” outputs/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
```

## WMT14 ë²ˆì—­ ëª¨ë¸ í•™ìŠµ (RTX 3090)

### 1. ë°ì´í„° ì¤€ë¹„

```bash
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
uv add datasets sentencepiece

# WMT14 ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
uv run scripts/prepare_wmt14_data.py --config configs/rtx3090.yaml

# ë””ë²„ê·¸ìš© ì‘ì€ ë°ì´í„°ì…‹ ì¤€ë¹„
uv run scripts/prepare_wmt14_data.py --config configs/rtx3090_debug.yaml
```

### 2. í•™ìŠµ ì‹¤í–‰

```bash
# RTX 3090ì— ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
uv run train_wmt14.py --config configs/rtx3090.yaml

# ë””ë²„ê·¸ ëª¨ë“œë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
uv run train_wmt14.py --debug

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
uv run train_wmt14.py --config configs/rtx3090.yaml --resume checkpoints/rtx3090/checkpoint-1000
```

### 3. RTX 3090 ìµœì í™” ì„¤ì •

**configs/rtx3090.yaml** ì£¼ìš” ì„¤ì •:
- **ë°°ì¹˜ í¬ê¸°**: 12 (ë¬¸ì¥ ë‹¨ìœ„)
- **Gradient Accumulation**: 20 steps (íš¨ê³¼ì  ë°°ì¹˜: 240)
- **Mixed Precision (FP16)**: í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- **ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ**: 100 í† í°
- **ë°ì´í„° ì„œë¸Œì…‹**: 100,000 ë¬¸ì¥ (ì „ì²´ 4.5M ì¤‘)

### 4. ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ ë‹¤ìŒ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤:
- ì†ì‹¤ê°’ (Loss)
- í•™ìŠµë¥  (Learning Rate)
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ì²˜ë¦¬ ì†ë„ (samples/sec)

TensorBoardë¡œ ìƒì„¸ ëª¨ë‹ˆí„°ë§:
```bash
tensorboard --logdir logs/rtx3090
```

### 5. ì˜ˆìƒ í•™ìŠµ ì‹œê°„

RTX 3090 ê¸°ì¤€:
- **ë””ë²„ê·¸ ëª¨ë“œ**: ì•½ 10-30ë¶„
- **100K ì„œë¸Œì…‹**: ì•½ 12-24ì‹œê°„
- **ì „ì²´ ë°ì´í„°ì…‹**: í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥ (ë©”ëª¨ë¦¬ ì œí•œ)

## ì°¸ê³  ë¬¸í—Œ
- Vaswani et al., ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762), 2017
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) (Harvard NLP)
